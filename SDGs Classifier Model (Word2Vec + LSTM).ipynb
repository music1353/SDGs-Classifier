{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load_data\n",
    "\n",
    "import glob\n",
    "\n",
    "ARTICLE_FOLDER = '../2018_research_data/article'\n",
    "LABEL_FOLDER = '../2018_research_data/label'\n",
    "\n",
    "article = []\n",
    "label = []\n",
    "\n",
    "for file in glob.glob(ARTICLE_FOLDER + \"/*.txt\"):\n",
    "    with open(file, \"r\") as f:\n",
    "        article.append(f.read())\n",
    "        \n",
    "for file in glob.glob(LABEL_FOLDER + \"/*.txt\"):\n",
    "    with open(file, \"r\") as f:\n",
    "        tags = f.read().split('\\n')[:-1]\n",
    "        label.append(tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprcoessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/jensonsu/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "class Preprocessing:\n",
    "    def __init__(self):        \n",
    "        self.stopwords = self._load_stopwords()\n",
    "        \n",
    "    def text_preprocessing(self, article_list):\n",
    "        token_list = self.tokenize(article_list)\n",
    "        # print('Successfully tokenized!')\n",
    "        \n",
    "        token_list = self.remove_stopwords(token_list)\n",
    "        # print('Successfully remove stopwords!')\n",
    "        \n",
    "        # token_list = self.select_POS(token_list)\n",
    "        # print('Successfully POS selected!')\n",
    "        \n",
    "        # lemmatization\n",
    "        token_list = self.lemmatization(token_list)\n",
    "        # print('Successfully lemmatization!')\n",
    "        \n",
    "        # concate tokens\n",
    "        result_list = []\n",
    "        for tokens in token_list:\n",
    "            content = ' '.join(tokens)\n",
    "            result_list.append(content)\n",
    "        return result_list\n",
    "        \n",
    "    def _load_stopwords(self):\n",
    "        sw = set(stopwords.words('english'))\n",
    "        my_stopwords = []\n",
    "        sw = list(sw) + my_stopwords\n",
    "        return sw\n",
    "    \n",
    "    def cut_sent(self, text:str):\n",
    "        '''將文章斷句為sent\n",
    "            Parameters\n",
    "                text {str} 一篇文章\n",
    "            Return {list} 被斷開的句子\n",
    "        '''\n",
    "        text = text.replace('\\n', '')\n",
    "        text = text.strip()\n",
    "        return text.split(\".\")\n",
    "    \n",
    "    def tokenize(self, article_list:list):\n",
    "        result_list = []\n",
    "        for article in article_list:\n",
    "            tokens = nltk.word_tokenize(article)\n",
    "            token_filtered = [w.lower() for w in tokens if w.isalpha()]\n",
    "            result_list.append(token_filtered)\n",
    "        return result_list\n",
    "    \n",
    "    def remove_stopwords(self, tokenized_article_list:list):\n",
    "        result_list = []\n",
    "        for tokens in tokenized_article_list:\n",
    "            result_list.append([w for w in tokens if w not in self.stopwords])\n",
    "        return result_list\n",
    "    \n",
    "    def lemmatization(self, tokenized_article_list:list):\n",
    "        wnl = WordNetLemmatizer()\n",
    "        result_list = []\n",
    "        for tokens in tokenized_article_list:\n",
    "            result_list.append([wnl.lemmatize(word) for word in tokens])\n",
    "        return result_list\n",
    "    \n",
    "    def select_POS(self, tokenized_article_list,\n",
    "                    selective_POS=['NN', 'NNS', 'NNP', 'NNPS', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'JJ']):\n",
    "        result_list = []\n",
    "        for tokens in tokenized_article_list:\n",
    "            pos_token_list = nltk.pos_tag(tokens)\n",
    "            \n",
    "            selective_pos_tkns = []\n",
    "            for tkn in pos_token_list:\n",
    "                if tkn[1] in selective_POS:\n",
    "                    selective_pos_tkns.append(tkn[0])\n",
    "            \n",
    "            result_list.append(selective_pos_tkns)\n",
    "        return result_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing\n",
    "\n",
    "prep = Preprocessing()\n",
    "article = prep.text_preprocessing(article)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''preprocessing corpus in seg_sent.txt'''\n",
    "\n",
    "ARTICLE_FOLDER = '../2018_research_data/article'\n",
    "SEG_SENT_PATH = './seg_sent.txt'\n",
    "\n",
    "with open(SEG_SENT_PATH, 'w', encoding='utf8') as output:\n",
    "    for file in glob.glob(ARTICLE_FOLDER + \"/*.txt\"):\n",
    "        with open(file, \"r\") as f:\n",
    "            text = f.read()\n",
    "            sent_list = prep.cut_sent(text)\n",
    "\n",
    "            for sent in sent_list:\n",
    "                sent = prep.text_preprocessing([sent])[0]\n",
    "                if sent is not '' and sent is not None and len(sent)>3:\n",
    "                    output.write(sent)\n",
    "                    output.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "import multiprocessing\n",
    "\n",
    "class Word2Vec(Preprocessing):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.ARTICLE_FOLDER = '../2018_research_data/article'\n",
    "        self.SEG_TEXT_PATH = './seg_sent.txt'\n",
    "        \n",
    "        self.vector_size = 50\n",
    "        self.window = 3\n",
    "        self.min_count = 0\n",
    "        self.sg = 0\n",
    "        self.negative= 5\n",
    "        \n",
    "        self.model = None\n",
    "        \n",
    "    def preprocess_and_save_seg_text(self):\n",
    "        with open(self.SEG_TEXT_PATH, 'w', encoding='utf8') as output:\n",
    "            for file in glob.glob(ARTICLE_FOLDER + \"/*.txt\"):\n",
    "                with open(file, \"r\") as f:\n",
    "                    text = f.read()\n",
    "                    sent_list = self.cut_sent(text)\n",
    "\n",
    "                    for sent in sent_list:\n",
    "                        sent = self.text_preprocessing([sent])[0]\n",
    "                        if sent is not '' and sent is not None and len(sent)>3:\n",
    "                            output.write(sent)\n",
    "                            output.write('\\n')\n",
    "        print('Successfully perprocess and save articles into seg text format.')\n",
    "                            \n",
    "    def train(self, vector_size=None, window=None, min_count=None, sg=None, negative=None):\n",
    "        if vector_size:\n",
    "            self.vector_size = vector_size\n",
    "        if window:\n",
    "            self.window = window\n",
    "        if min_count:\n",
    "            self.min_count = min_count\n",
    "        if sg:\n",
    "            self.sg = sg\n",
    "        if negative:\n",
    "            self.negative = negative\n",
    "        \n",
    "        sentences = word2vec.LineSentence(self.SEG_TEXT_PATH)\n",
    "        self.model = word2vec.Word2Vec(sentences, size=self.vector_size, window=self.window,\n",
    "                                           min_count=self.min_count, workers=multiprocessing.cpu_count(),\n",
    "                                           sg=self.sg, negative=self.negative)\n",
    "        print('Successfully train word2vec model!')\n",
    "        \n",
    "    def save(self, path='./w2v_model'):\n",
    "        self.model.save(path+\".model\")\n",
    "        print('Successfully save word2vec model!')\n",
    "        \n",
    "    def load(self, path='./w2v_model'):\n",
    "        self.model = word2vec.Word2Vec.load(path+\".model\")\n",
    "        print('Successfully load word2vec model!')\n",
    "        \n",
    "    def map_word_2_embedding(self):\n",
    "        embedding_index = dict()\n",
    "        for word, embed in zip(self.model.wv.index2word, self.model.wv.vectors):\n",
    "            embedding_index[word] = embed\n",
    "        return embedding_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = Word2Vec()\n",
    "w2v.preprocess_and_save_seg_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indx = w2v.map_word_2_embedding()\n",
    "w2v.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras LSTM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from sklearn import model_selection\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers.core import Dense, Activation, Flatten, Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import LSTM\n",
    "\n",
    "\n",
    "class lstm_SDGs_classifier:\n",
    "    def __init__(self):\n",
    "        # data folder\n",
    "        self.ARTICLE_FOLDER = '../2018_research_data/article'\n",
    "        self.LABEL_FOLDER = '../2018_research_data/label'\n",
    "        \n",
    "        # raw data\n",
    "        self._articles = [] # raw articles data\n",
    "        self._labels = [] # raw labels data\n",
    "        \n",
    "        # training data\n",
    "        self._train_text = []\n",
    "        self._train_y_one_hot = []\n",
    "        \n",
    "        # valid data\n",
    "        self._valid_text = []\n",
    "        self._valid_y_one_hot = []\n",
    "        \n",
    "        # token\n",
    "        self.tokenizer = []\n",
    "        self.vocab_size = 0\n",
    "        \n",
    "        # bag of words\n",
    "        self._embed_dim = 150 # 詞向量維度\n",
    "        self._feature_dim = 500 # 字典內的單字數\n",
    "        self._max_len = 100 # 每句話最大長度\n",
    "        \n",
    "        # model\n",
    "        self.model = None\n",
    "        \n",
    "    def load_data(self, train_data=[], valid_data=[]):\n",
    "        # load memory data\n",
    "        if train_data and valid_data:\n",
    "            self._articles = train_data\n",
    "            self._labels = valid_data\n",
    "        else:\n",
    "            for file in glob.glob(self.ARTICLE_FOLDER + \"/*.txt\"):\n",
    "                with open(file, \"r\") as f:\n",
    "                    self._articles.append(f.read())\n",
    "\n",
    "            for file in glob.glob(self.LABEL_FOLDER + \"/*.txt\"):\n",
    "                with open(file, \"r\") as f:\n",
    "                    tags = f.read().split('\\n')[:-1]\n",
    "                    self._labels.append(tags)\n",
    "                \n",
    "        print('Successfully load data.')\n",
    "                \n",
    "    def preprocessing(self, feature_dim=1000, max_len=100):\n",
    "        # set feature_dim & max_len\n",
    "        self._feature_dim = feature_dim\n",
    "        self._max_len = max_len\n",
    "        \n",
    "        # split the dataset into training and testing \n",
    "        train_x, valid_x, train_y, valid_y = model_selection.train_test_split(self._articles,\n",
    "                                                        self._labels, test_size=0.1, random_state=1)\n",
    "        \n",
    "        # build token\n",
    "        self.tokenizer = Tokenizer(num_words=self._feature_dim)\n",
    "        self.tokenizer.fit_on_texts(train_x)\n",
    "        self.vocab_size = len(self.tokenizer.word_index) + 1\n",
    "        print('vocab_size:', self.vocab_size)\n",
    "        \n",
    "        # pre-process train & valid x: text to seq & pad seq\n",
    "        self._train_text = self._text_vectorization(train_x)\n",
    "        self._valid_text = self._text_vectorization(valid_x)\n",
    "        \n",
    "        # pre-process train & valid y: label one-hot encoding\n",
    "        self._train_y_one_hot = self._label_oneHot_encoding(train_y)\n",
    "        self._valid_y_one_hot = self._label_oneHot_encoding(valid_y)\n",
    "        \n",
    "        print('Successfully pre-process the raw data.')\n",
    "    \n",
    "    def set_up_model(self, embed_dim=150, embedding_matrix=None):\n",
    "        self._embed_dim = embed_dim\n",
    "        \n",
    "        model = Sequential()\n",
    "        \n",
    "        if embedding_matrix is not None:\n",
    "            print('load in pre-train embedding.')\n",
    "            model.add(Embedding(input_dim=self.vocab_size, output_dim=self._embed_dim, \n",
    "                                weights=[embedding_matrix], trainable=False, input_length=self._max_len))\n",
    "        else:\n",
    "            model.add(Embedding(input_dim=self.vocab_size, output_dim=self._embed_dim, input_length=self._max_len))\n",
    "        \n",
    "        \n",
    "        model.add(LSTM(128))\n",
    "        model.add(Dense(units=128, activation='relu'))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Dense(units=128, activation='relu'))\n",
    "        model.add(Dense(units=17, activation='sigmoid'))\n",
    "        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        model.summary()\n",
    "        self.model = model\n",
    "        \n",
    "        print('Successfully set up model, everything is already!')\n",
    "        \n",
    "    def train(self, batch_size=256, epochs=50, verbose=2, shuffle=True, validation_split=0.1):\n",
    "        train_history = self.model.fit(self._train_text, self._train_y_one_hot, batch_size=batch_size,\n",
    "                            epochs=epochs, verbose=verbose, shuffle=shuffle, validation_split=validation_split)\n",
    "        return train_history\n",
    "    \n",
    "    def show_train_history(self, train_history, train, validation):\n",
    "        plt.plot(train_history.history[train])\n",
    "        plt.plot(train_history.history[validation])\n",
    "        plt.title('Train History')\n",
    "        plt.ylabel(train)\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.legend(['train', 'validation'], loc='upper left')\n",
    "        plt.show()\n",
    "        \n",
    "    def evaluate_model(self):\n",
    "        scores = self.model.evaluate(self._valid_text, self._valid_y_one_hot)\n",
    "        print('Accuracy:', scores[1])\n",
    "        \n",
    "    def save_tokenizer(self, tokenizer_path='my_tokenizer'):\n",
    "        with open(tokenizer_path+'.pickle', 'wb') as handle:\n",
    "            pickle.dump(self.tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        print('Successfully save tokenizer')\n",
    "        \n",
    "    def load_tokenizer(self, tokenizer_path):\n",
    "        with open(tokenizer_path+'.pickle', 'rb') as handle:\n",
    "            self.tokenizer = pickle.load(handle)\n",
    "        \n",
    "    def save_model(self, model_path='my_model'):\n",
    "        self.model.save(model_path)\n",
    "        print('Successfully save model')\n",
    "        \n",
    "    def load_model(self, model_path):\n",
    "        self.model = load_model(model_path)\n",
    "        \n",
    "    def predict(self, text):\n",
    "        vector_text = self._text_vectorization([text])\n",
    "        # print('vector_text:', vector_text)\n",
    "        predict_soft = self.model.predict(vector_text)\n",
    "        \n",
    "        result = self._pred_2_goal(predict_soft)\n",
    "        return result\n",
    "        \n",
    "    def _text_vectorization(self, text):\n",
    "        text_seq = self.tokenizer.texts_to_sequences(text)\n",
    "        # print('text_seq:', text_seq)\n",
    "        return sequence.pad_sequences(text_seq, maxlen=self._max_len)\n",
    "        \n",
    "    def _label_oneHot_encoding(self, labels, label_dim=17):\n",
    "        one_hot_label = []\n",
    "        for label in labels:\n",
    "            arr = label_dim*[float(0)]\n",
    "            for i in label:\n",
    "                arr[int(i)-1] = float(1)\n",
    "            one_hot_label.append(arr)\n",
    "        return np.array(one_hot_label)\n",
    "    \n",
    "    def _pred_2_goal(self, pred_result):\n",
    "        obj = {}\n",
    "        for i in range(1, 18):\n",
    "            obj[i] = pred_result[0][i-1]\n",
    "        \n",
    "        sorted_list = sorted(obj.items(), key=lambda k: k[1], reverse=True) \n",
    "        return sorted_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_classifier = lstm_SDGs_classifier()\n",
    "\n",
    "lstm_classifier.load_data(article, label)\n",
    "lstm_classifier.preprocessing(feature_dim=500, max_len=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''load in wrod2vec pre-train embedding'''\n",
    "\n",
    "embed_dim = 50\n",
    "\n",
    "# create a weight matrix for words in training docs\n",
    "embedding_matrix = np.zeros((lstm_classifier.vocab_size, embed_dim))\n",
    "for word, index in lstm_classifier.tokenizer.word_index.items():\n",
    "    embedding_vector = embedding_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[index] = embedding_vector\n",
    "\n",
    "# print(embedding_matrix)\n",
    "\n",
    "lstm_classifier.set_up_model(embed_dim=embed_dim, embedding_matrix=embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = lstm_classifier.train(batch_size=128, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_classifier.show_train_history(history, train='accuracy', validation='val_accuracy')\n",
    "lstm_classifier.show_train_history(history, train='loss', validation='val_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_classifier.evaluate_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''This study separates from two major aspects, and one is industry expanding, another is business operation. According to the work experiences and the professionals’ comments, these two aspects will influence stock price. First, this study investigates the overall review and uses the Diamond model, and finds out which elements will change in the same direction with the product value of Taiwan IC design industry. And this aspect generalizes 11 industry expanding factors. Second, my research uses the five force analysis model and value chain model, and finds out which elements will change in the same direction with the EPS of MediaTek. And this aspect generalizes 29 corporate’s earning growth factors.In the end, my research selects 19 impact factor of stock price trend. And these are selected from the two aspects, which will change in the same direction with the stock price. According to the 19 factors, this study constructs an analysis framework. This framework concludes two aspects and ten elements. One is the industry aspect, and its elements are global industry, Taiwan industry, related industries, and product demand. Another is the corporate aspect, and its elements are enterprise profit, business operations, core of competition, major competitors, major customers, and major suppliers.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing text\n",
    "text = prep.text_preprocessing([text])[0]\n",
    "print(text)\n",
    "\n",
    "lstm_classifier.predict(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_classifier.save_model('20201027_w2v_lstm_model_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_classifier.save_tokenizer('20201027_token')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model & Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''This study separates from two major aspects, and one is industry expanding, another is business operation. According to the work experiences and the professionals’ comments, these two aspects will influence stock price. First, this study investigates the overall review and uses the Diamond model, and finds out which elements will change in the same direction with the product value of Taiwan IC design industry. And this aspect generalizes 11 industry expanding factors. Second, my research uses the five force analysis model and value chain model, and finds out which elements will change in the same direction with the EPS of MediaTek. And this aspect generalizes 29 corporate’s earning growth factors.In the end, my research selects 19 impact factor of stock price trend. And these are selected from the two aspects, which will change in the same direction with the stock price. According to the 19 factors, this study constructs an analysis framework. This framework concludes two aspects and ten elements. One is the industry aspect, and its elements are global industry, Taiwan industry, related industries, and product demand. Another is the corporate aspect, and its elements are enterprise profit, business operations, core of competition, major competitors, major customers, and major suppliers.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully tokenized!\n",
      "Successfully remove stopwords!\n",
      "Successfully lemmatization!\n",
      "study separate two major aspect one industry expanding another business operation according work experience professional comment two aspect influence stock price first study investigates overall review us diamond model find element change direction product value taiwan ic design industry aspect generalizes industry expanding factor second research us five force analysis model value chain model find element change direction eps mediatek aspect generalizes corporate earning growth end research selects impact factor stock price trend selected two aspect change direction stock price according factor study construct analysis framework framework concludes two aspect ten element one industry aspect element global industry taiwan industry related industry product demand another corporate aspect element enterprise profit business operation core competition major competitor major customer major supplier\n",
      "[[0.0212816  0.00760403 0.15945989 0.1329284  0.01687202 0.01813936\n",
      "  0.06425646 0.24569705 0.5381969  0.05701056 0.22825849 0.12108561\n",
      "  0.03689823 0.01472878 0.06441569 0.08298901 0.20907578]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(9, 0.5381969),\n",
       " (8, 0.24569705),\n",
       " (11, 0.22825849),\n",
       " (17, 0.20907578),\n",
       " (3, 0.15945989),\n",
       " (4, 0.1329284),\n",
       " (12, 0.121085614),\n",
       " (16, 0.08298901),\n",
       " (15, 0.06441569),\n",
       " (7, 0.06425646),\n",
       " (10, 0.05701056),\n",
       " (13, 0.036898226),\n",
       " (1, 0.0212816),\n",
       " (6, 0.018139362),\n",
       " (5, 0.016872019),\n",
       " (14, 0.014728785),\n",
       " (2, 0.0076040328)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preprocessing text\n",
    "prep = Preprocessing()\n",
    "text = prep.text_preprocessing([text])[0]\n",
    "print(text)\n",
    "\n",
    "# load model\n",
    "lc = lstm_SDGs_classifier()\n",
    "lc.load_tokenizer('20201027_token')\n",
    "lc.load_model('20201027_w2v_lstm_model_2')\n",
    "\n",
    "# predict\n",
    "lc.predict(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SDGs Classifier Percision Calculate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo\n",
    "\n",
    "MONGO_URI= 'mongodb://admin:mongoadmin@35.201.137.113:27017/?authSource=admin&readPreference=primary&appname=MongoDB%20Compass&ssl=false'\n",
    "client = pymongo.MongoClient(MONGO_URI, connect=False)\n",
    "db = client['nthu_sdg_db']\n",
    "\n",
    "user_collect = db['users']\n",
    "paper_collect = db['papers']\n",
    "\n",
    "robot_paper_collect = db['robot_papers']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all papers\n",
    "\n",
    "agg_doc = user_collect.aggregate([\n",
    "            {\n",
    "                '$lookup': {\n",
    "                    'from': \"papers\",\n",
    "                    'localField': \"account\",\n",
    "                    'foreignField': \"account\",\n",
    "                    'as': \"paper_doc\"\n",
    "                }\n",
    "            }\n",
    "        ])\n",
    "agg_doc_list = list(agg_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Evaluate(Preprocessing):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.college = ['COTM', 'EECS', 'ENGI', 'CHS', 'NUCL', 'THC', 'SCI', 'LSCO', 'HCTC', 'TE', 'OAA', 'CARS']\n",
    "        self.evalute_dict = {}\n",
    "        \n",
    "    def set_evaluate_format(self, format=[]):\n",
    "        self.evalute_dict = {}\n",
    "        for c in self.college:\n",
    "            self.evalute_dict[c] = {}\n",
    "            for f in format:\n",
    "                self.evalute_dict[c][f] = 0\n",
    "        print('Successfully set evaluate dict.')\n",
    "        \n",
    "    def calculate_hit(self, classifier, paper_list):\n",
    "        '''預測的前三項goal，命中使用者選擇goal的比例'''\n",
    "        total_hit_count = 0\n",
    "        \n",
    "        # calculate paper\n",
    "        for paper in paper_list:\n",
    "            text = paper['summaryEN']\n",
    "            text = self.text_preprocessing([text])[0]\n",
    "\n",
    "            user_tags = [ str(t['goal_id']) for t in paper['tags'] ]\n",
    "            pred_tags = [ str(t[0]) for t in classifier.predict(text)[:3] ]\n",
    "\n",
    "            # 預測的前三項goal，命中使用者選擇goal的比例\n",
    "            for t in user_tags:\n",
    "                if t in pred_tags:\n",
    "                    total_hit_count += 1\n",
    "                    self.evalute_dict[paper['college']]['hit_count'] += 1\n",
    "                    break\n",
    "            self.evalute_dict[paper['college']]['paper_count'] += 1\n",
    "            \n",
    "        # calculate percision\n",
    "        for col in self.evalute_dict.keys():\n",
    "            try:\n",
    "                hit_count = self.evalute_dict[col]['hit_count']\n",
    "                paper_count = self.evalute_dict[col]['paper_count']\n",
    "                self.evalute_dict[col]['percision'] = round(hit_count/paper_count*100, 2)\n",
    "            except Exception as err:\n",
    "                print('Col:', col)\n",
    "                print('Col Dict:', self.evalute_dict[col])\n",
    "                print('Err:', err)\n",
    "        self.evalute_dict['percision'] = round(total_hit_count/len(paper_list)*100, 2)\n",
    "    \n",
    "    def percision(self, classifier, paper_list, mode='hit'):\n",
    "        if mode is 'hit':\n",
    "            self.calculate_hit(classifier, paper_list)\n",
    "        return self.evalute_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 取出所有已標記papers\n",
    "\n",
    "paper_list = []\n",
    "\n",
    "for user in agg_doc_list:\n",
    "    if user['account']=='music1353@gmail.com':\n",
    "        continue\n",
    "    \n",
    "    papers = user['paper_doc'][0]['papers']\n",
    "    college = user['college']\n",
    "    \n",
    "    for p in papers:\n",
    "        if p['isTag'] is False:\n",
    "            continue\n",
    "        \n",
    "        p['college'] = college\n",
    "        paper_list.append(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully set evaluate dict.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'COTM': {'paper_count': 440, 'hit_count': 253, 'percision': 57.5},\n",
       " 'EECS': {'paper_count': 600, 'hit_count': 431, 'percision': 71.83},\n",
       " 'ENGI': {'paper_count': 1025, 'hit_count': 741, 'percision': 72.29},\n",
       " 'CHS': {'paper_count': 208, 'hit_count': 102, 'percision': 49.04},\n",
       " 'NUCL': {'paper_count': 527, 'hit_count': 332, 'percision': 63.0},\n",
       " 'THC': {'paper_count': 94, 'hit_count': 68, 'percision': 72.34},\n",
       " 'SCI': {'paper_count': 365, 'hit_count': 236, 'percision': 64.66},\n",
       " 'LSCO': {'paper_count': 309, 'hit_count': 212, 'percision': 68.61},\n",
       " 'HCTC': {'paper_count': 545, 'hit_count': 374, 'percision': 68.62},\n",
       " 'TE': {'paper_count': 6, 'hit_count': 5, 'percision': 83.33},\n",
       " 'OAA': {'paper_count': 2, 'hit_count': 1, 'percision': 50.0},\n",
       " 'CARS': {'paper_count': 35, 'hit_count': 13, 'percision': 37.14},\n",
       " 'percision': 66.6}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate = Evaluate()\n",
    "evaluate.set_evaluate_format(['paper_count', 'hit_count', 'percision'])\n",
    "\n",
    "lc = lstm_SDGs_classifier()\n",
    "lc.load_tokenizer('20201027_token')\n",
    "lc.load_model('20201027_w2v_lstm_model_2')\n",
    "\n",
    "result = evaluate.percision(lc, paper_list, mode='hit')\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
